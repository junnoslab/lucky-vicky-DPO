from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("yanolja/EEVE-Korean-10.8B-v1.0")
tokenizer = AutoTokenizer.from_pretrained("yanolja/EEVE-Korean-10.8B-v1.0")

INSTRUCTION_TEMPLATE: str = "\n### Instruction:"
QUESTION_TEMPLATE: str = "\n### Question:"
ANSWER_TEMPLATE: str = "\n### Answer:"

PROMPT_TEMPLATE: str = """
### Instruction:
ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ë¶ˆì•ˆí•˜ê±°ë‚˜ ë¶ˆí–‰í•œ ì¼ì„ í‘œí˜„í•˜ëŠ” ë¬¸ì¥ì„ ê¸ì •ì ì¸ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
- ì²« ë¬¸ì¥ì—ëŠ” ìƒí™©ê³¼ ê°ì •ì„ í‘œí˜„í•œë‹¤.
- ë‘ ë²ˆì§¸ ë¬¸ì¥ì—ëŠ” í˜„ì¬ ìƒí™©ë³´ë‹¤ ë” ì¢‹ê±°ë‚˜ ëœ ì¢‹ì€ ìƒí™©ì„ ì„¤ëª…í•œë‹¤.
- ì„¸ë²ˆì§¸ ë¬¸ì¥ì—ëŠ” ë” ì¢‹ê±°ë‚˜ ëœ ì¢‹ì€ ìƒí™© ëª¨ë‘ ë³„ë¡œë‹ˆê¹Œ ê²°ë¡ ì ìœ¼ë¡œ í˜„ì¬ ìƒí™©ì´ ê°€ì¥ ê¸ì •ì ì´ë¼ëŠ” ê²ƒì„ ì„¤ëª…í•œë‹¤.
- ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ê³ ì •ì ìœ¼ë¡œ 'ì™„ì „ ëŸ­í‚¤ë¹„í‚¤ì”ì•™ğŸ€'ì„ ì‚¬ìš©í•´ ë§ˆì¹œë‹¤.
- ê°íƒ„ì‚¬ì™€ ì´ëª¨ì§€ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.

### Question:
{QUESTION}

### Answer:
{ANSWER}
"""

PROMPT = PROMPT_TEMPLATE.format(
    QUESTION="ë‚´ ì•ì—ì„œ 50% ì„¸ì¼í•˜ëŠ” ì˜·ì´ í’ˆì ˆëì–´", ANSWER=""
)


def print_tokens_with_ids(txt):
    tokens = tokenizer.tokenize(txt, add_special_tokens=False)
    token_ids = tokenizer.encode(txt, add_special_tokens=False)
    print(list(zip(tokens, token_ids)))


print_tokens_with_ids(PROMPT)
print_tokens_with_ids(INSTRUCTION_TEMPLATE)
print_tokens_with_ids(ANSWER_TEMPLATE)

instruction_template_ids = tokenizer.encode(
    INSTRUCTION_TEMPLATE, add_special_tokens=False
)[2:]
answer_template_ids = tokenizer.encode(ANSWER_TEMPLATE, add_special_tokens=False)[2:]
